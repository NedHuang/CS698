{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import importlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uncomment these to use the solution instead of your own implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from a2_solutions import FeedForward\n",
    "# from a2_solutions import BackProp\n",
    "# from a2_solutions import Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Logistic Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma(z) = \\frac{1}{1-e^{-z}}\n",
    "\\\\\n",
    "\\frac{\\partial (1+e^{-z})}{\\partial z} = -(e^{-z}) = -e^{-z}\n",
    "\\\\\n",
    "\\frac{\\partial \\sigma(z)}{\\partial z} = \n",
    "\\frac{ \n",
    "    \\frac{\\partial 1}{\\partial z} \\cdot (1+e^{-z})  -\n",
    "    \\frac{\\partial (1+e^{-z})}{\\partial z} \\cdot 1\n",
    "}\n",
    "{\\sigma(z)^{2}}\n",
    "=\n",
    "\\frac{e^{-z}}{(1+e^{-z})^2}\n",
    "\\\\\n",
    "=\n",
    "\\frac{1}{(1+e^{-z})}\\frac{(1+e^{-z})-1}{(1+e^{-z})} = \n",
    "\\frac{1}{(1+e^{-z})}\\left (1-\\frac{(1}{(1+e^{-z})}\\right )= \\sigma(z)(1-\\sigma(z))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help you with $\\LaTeX$, and to show you my expectations, here is a sample taken from the lecture notes, taken from the 3rd and 4th page of the notes entitled \"Error Backpropagation\". It has nothing to do with the solution to this question, but just demonstrates some of the features of $\\LaTeX$. Notice how I include English statments to guide the reader through the derivation.\n",
    "\n",
    "<a target=_new href=\"http://detexify.kirelabs.org/classify.html\">This web page</a> is very handy for identifying $\\LaTeX$ symbols.\n",
    "\n",
    "---\n",
    "More generally, for $\\vec{x} \\in \\mathbb{R}^X$, $\\vec{h} \\in \\mathbb{R}^H$, and $\\vec{y} \\in \\mathbb{R}^Y$.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial \\alpha_i}\n",
    "  &= \\frac{d h_i}{d \\alpha_i} \\\\\n",
    "  &= \\frac{d h_i}{d \\alpha_i}\n",
    "  \\left[ M_{1i} \\ \\cdots \\ M_{Yi} \\right] \\cdot\n",
    "  \\left[ \\frac{\\partial E}{\\partial \\beta_1} \\ \\cdots \\ \\frac{\\partial E}{\\partial \\beta_Y} \\right] \\\\\n",
    "  &= \\frac{d h_i}{d \\alpha_i}\n",
    "   \\left[ M_{1i} \\ \\cdots \\ M_{Yi} \\right]\n",
    "   \\left[ \\begin{array}{c}\n",
    "     \\frac{\\partial E}{\\partial \\beta_1} \\\\\n",
    "     \\vdots \\\\\n",
    "     \\frac{\\partial E}{\\partial \\beta_Y} \\end{array} \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "Thus, for all elements,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left[ \\begin{array}{c}\n",
    "  \\frac{\\partial E}{\\partial \\alpha_1} \\\\\n",
    "  \\vdots \\\\\n",
    "  \\frac{\\partial E}{\\partial \\alpha_H}\n",
    "\\end{array} \\right] &=\n",
    "%\n",
    "\\left[ \\begin{array}{c}\n",
    "  \\frac{d h_1}{d \\alpha_1} \\\\\n",
    "  \\vdots \\\\\n",
    "  \\frac{d h_H}{d \\alpha_H}\n",
    "\\end{array} \\right]\n",
    "\\odot\n",
    "\\left[ \\begin{array}{ccc}\n",
    "  M_{11} & \\cdots & M_{Y1} \\\\\n",
    "  \\vdots & \\ddots & \\vdots \\\\\n",
    "  M_{1H} & \\cdots & M_{YH}\n",
    "\\end{array} \\right]\n",
    "%\n",
    "\\left[ \\begin{array}{c}\n",
    "  \\frac{\\partial E}{\\partial \\beta_1} \\\\\n",
    "  \\vdots \\\\\n",
    "  \\frac{\\partial E}{\\partial \\beta_Y}\n",
    "\\end{array} \\right] \\\\\n",
    "%\n",
    "\\frac{\\partial E}{\\partial \\vec{\\alpha}} &=\n",
    "\\frac{d \\vec{h}}{d \\vec{\\alpha}} \\odot M^\\mathrm{T}\n",
    "\\frac{\\partial E}{\\partial \\vec{\\beta}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q2: Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E\\left( \\overrightarrow{y},\\overrightarrow{t} \\right) = -{\\sum_{k=1}^{K}t_{k} \\ln{y_{k}}}\n",
    "\\\\\n",
    "y_{k} = \\frac{e^{z_{k}}}{\\sum_{j=1}^{K}e^{z_{j}}} \n",
    "\\\\\n",
    "\\frac{\\partial \\sum_{j=1}^{K}e^{z_{j}}}{\\partial z_{j}} = \\frac{\\partial \\sum_{k=1}^{K}e^{z_{k}}}{e^{z_{j}}}\n",
    "\\frac{e^{z_{j}}}{z_{j}} = 1 \\cdot e^{z_{j}} = e^{z_{j}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial y_{k}}{\\partial z_{j}} \n",
    "= \\frac{\n",
    "  \\frac{\\partial e^{z_{k}}}{\\partial z_{j}}   \\left (\\sum_{j=1}^{K}e^{z_{j}} \\right ) - \n",
    "  \\left(e^{z_{k}} \\right)\\frac{\\partial \\sum_{j=1}^{K}e^{z_{j}}}{\\partial z_{j}}\n",
    "}\n",
    "{\n",
    "  \\left (\\sum_{j=1}^{K} e^{z_{j}} \\right )^{2}\n",
    "}\n",
    "=  \\begin{cases} \n",
    "      \\frac{0\\cdot \\left (\\sum_{j=1}^{K}e^{z_{j}} - e^{z_{k}}\\right ) - e^{z_{k}}e^{z_{j}}}{\\left (\\sum_{j=1}^{K} e^{z_{j}} \\right )^{2}} \n",
    "       = \\frac{- e^{z_{k}}e^{z_{j}}}{\\left (\\sum_{j=1}^{K} e^{z_{j}} \\right )^{2}} = -y_{j}y_{k} & j\\neq k \\\\\n",
    "      \\\\\n",
    "      \\frac{e^{z_{j}} \\sum_{k=1}^{j}e^{z_{j}} - e^{z_{j}}} {\\left (\\sum_{j=1}^{K} e^{z_{j}} \\right )^{2}}\n",
    "      = \\frac{e^{2z_{j}}} {\\left (\\sum_{j=1}^{K} e^{z_{j}} \\right )^{2}}-\\frac{e^{z_{j}} } {\\left (\\sum_{j=1}^{K} e^{z_{j}} \\right )^{2}} = y_{j}(1-y_{j}) & j = k\n",
    "   \\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial E}{\\partial z_{j}} = -\\sum^{K}_{k=1} t_{k} \\frac{\\partial \\ln{y_{k}}}{\\partial z_{j}} = -\\sum^{K}_{k=1} t_{k} \\frac{\\partial \\ln{y_{k}}}{\\partial y_{k}}\\frac{\\partial y_{k}}{\\partial z_{j}} = -\\sum_{k=1}^{K} \\frac{t_{k}}{y_{k}} \\frac{\\partial y_{k}}{\\partial z_{j}}\n",
    "= - \\left[ \\left(\\sum_{k=1}^{K}\\frac{t_{k}}{y_{k}} \\left(-y_{j}y_{k}\\right) \\right) + \\frac{t_{j}}{y_{j}}y_{j} \\right] = y_{j}\\sum_{k=1}^{K}t_{k}-t_{j} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q3: Top-Layer Error Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## a\n",
    "$$\n",
    "\\frac{\\partial E(y,t)}{\\partial y} = \\frac{\\partial }{\\partial y} \\left (-t\\ln{y} -(1-t)\\ln(1-y)\\right ) = \\frac{-t}{y} - \\frac{1-t}{1-y}(-1) = \\frac{y-t}{y(1-y)}\\\\\n",
    "\\frac{\\partial y}{\\partial z} =\\sigma(z)\\left ( 1-\\sigma(z)\\right )\\\\\n",
    "\\frac{\\partial E}{\\partial z} =\\frac{\\partial E}{\\partial y}\\frac{\\partial y}{\\partial z} = \\frac{\\sigma(z)-t}{\\sigma(z)(1-\\sigma(z))}\n",
    "\\left[\\sigma(z)\\left ( 1-\\sigma(z)\\right )\\right] = \\sigma(z)-t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b\n",
    "$$\n",
    "\\frac{\\partial E(y,t)}{\\partial y} = \\frac{\\partial }{\\partial y} \\frac{1}{2} \\left ( y-t \\right )^2 = 2(y-t)\n",
    "\\\\\n",
    "y=\\sigma(z) = z\n",
    "\\\\\n",
    "\\frac{\\partial y}{\\partial z} =1\n",
    "\\\\\n",
    "\\frac{\\partial E}{\\partial z} =\\frac{1}{2}\\frac{\\partial E}{\\partial y}\\frac{\\partial y}{\\partial z} = (y-t) = (z-t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Q4: Implementing Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplied Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Supplied functions\n",
    "\n",
    "def NSamples(x):\n",
    "    '''\n",
    "        n = NSamples(x)\n",
    "        \n",
    "        Returns the number of samples in a batch of inputs.\n",
    "        \n",
    "        Input:\n",
    "         x   is a 2D array\n",
    "        \n",
    "        Output:\n",
    "         n   is an integer\n",
    "    '''\n",
    "    return len(x)\n",
    "\n",
    "def OneHot(z):\n",
    "    '''\n",
    "        y = OneHot(z)\n",
    "\n",
    "        Applies the one-hot function to the vectors in z.\n",
    "        Example:\n",
    "          OneHot([[0.9, 0.1], [-0.5, 0.1]])\n",
    "          returns np.array([[1,0],[0,1]])\n",
    "\n",
    "        Input:\n",
    "         z    is a 2D array of samples\n",
    "\n",
    "        Output:\n",
    "         y    is an array the same shape as z\n",
    "    '''\n",
    "    y = []\n",
    "    # Locate the max of each row\n",
    "    for zz in z:\n",
    "        idx = np.argmax(zz)\n",
    "        b = np.zeros_like(zz)\n",
    "        b[idx] = 1.\n",
    "        y.append(b)\n",
    "    y = np.array(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grading:\n",
    "# [1] Divide each by NSamples(t) or NSamples(y) to get the mean\n",
    "# Plus one mark for each of the 4 formulas, as indicated below.\n",
    "def CrossEntropy(y, t):\n",
    "    '''\n",
    "        E = CrossEntropy(y, t)\n",
    "\n",
    "        Evaluates the mean cross entropy loss between outputs y and targets t.\n",
    "\n",
    "        Inputs:\n",
    "          y is an array holding the network outputs\n",
    "          t is an array holding the corresponding targets\n",
    "\n",
    "        Outputs:\n",
    "          E is the mean CE\n",
    "    '''\n",
    "    \n",
    "    # === YOUR CODE HERE ===\n",
    "    \n",
    "    return - np.sum(t * np.log(y) + (1-t)* np.log(1-y))/ NSamples(y)\n",
    "\n",
    "\n",
    "def gradCrossEntropy(y, t):\n",
    "    '''\n",
    "        E = gradCrossEntropy(y, t)\n",
    "\n",
    "        Given targets t, evaluates the gradient of the mean cross entropy loss\n",
    "        with respect to the output y.\n",
    "\n",
    "        Inputs:\n",
    "          y is the array holding the network's output\n",
    "          t is an array holding the corresponding targets\n",
    "\n",
    "        Outputs:\n",
    "          dEdy is the gradient of CE with respect to output y\n",
    "    '''\n",
    "    \n",
    "    # === YOUR CODE HERE ===\n",
    "    return ((y-t)/(y*(1-y))) / NSamples(y)\n",
    "    \n",
    "\n",
    "def MSE(y, t):\n",
    "    '''\n",
    "        E = MSE(y, t)\n",
    "\n",
    "        Evaluates the mean squared error loss between outputs y and targets t.\n",
    "\n",
    "        Inputs:\n",
    "          y is the array holding the network's output\n",
    "          t is an array holding the corresponding targets\n",
    "\n",
    "        Outputs:\n",
    "          E is the MSE\n",
    "    '''\n",
    "    \n",
    "    # === YOUR CODE HERE ===\n",
    "\n",
    "    return 1 / 2 * np.sum((y - t)**2) / NSamples(y)   \n",
    "    \n",
    "\n",
    "def gradMSE(y, t):\n",
    "    '''\n",
    "        E = gradMSE(y, t)\n",
    "\n",
    "        Given targets t, evaluates the gradient of the mean squared error loss\n",
    "        with respect to the output y.\n",
    "\n",
    "        Inputs:\n",
    "          y is the array holding the network's output\n",
    "          t is an array holding the corresponding targets\n",
    "\n",
    "        Outputs:\n",
    "          dEdy is the gradient of MSE with respect to output y\n",
    "    '''\n",
    "    \n",
    "    # === YOUR CODE HERE ===\n",
    "    return (y - t)/ NSamples(y)   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================\n",
    "#\n",
    "#  UNCOMMENT THE CORRESPONDING LINES BELOW IF YOU WANT TO USE\n",
    "#  THE SOLUTIONS INSTEAD OF YOUR VERSION.\n",
    "#\n",
    "#================================================================\n",
    "# from a2_solutions import CrossEntropy\n",
    "# from a2_solutions import gradCrossEntropy\n",
    "# from a2_solutions import MSE\n",
    "# from a2_solutions import gradMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \n",
    "    def __init__(self, n_nodes, act='logistic'):\n",
    "        '''\n",
    "            lyr = Layer(n_nodes, act='logistic')\n",
    "            \n",
    "            Creates a layer object.\n",
    "            \n",
    "            Inputs:\n",
    "             n_nodes  the number of nodes in the layer\n",
    "             act      specifies the activation function\n",
    "                      Use 'logistic' or 'identity'\n",
    "        '''\n",
    "        self.N = n_nodes  # number of nodes in this layer\n",
    "        self.h = []       # node activities\n",
    "        self.z = []\n",
    "        self.b = np.zeros(self.N)  # biases\n",
    "        \n",
    "        # Activation functions\n",
    "        self.sigma = self.Logistic\n",
    "        self.sigma_p = (lambda : self.Logistic_p())\n",
    "        if act == 'identity':\n",
    "            self.sigma = self.Identity\n",
    "            self.sigma_p = (lambda : self.Identity_p())\n",
    "       \n",
    "    def Logistic(self):\n",
    "        return 1. / (1. + np.exp(-self.z))\n",
    "    def Logistic_p(self):\n",
    "        return self.h * (1.-self.h)\n",
    "    def Identity(self):\n",
    "        return self.z\n",
    "    def Identity_p(self):\n",
    "        return np.ones_like(self.h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4(b,c,d) Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     52,
     78,
     94,
     106,
     126
    ]
   },
   "outputs": [],
   "source": [
    "class Network():\n",
    "\n",
    "    \n",
    "    def FeedForward(self, x):\n",
    "        '''\n",
    "            y = net.FeedForward(x)\n",
    "\n",
    "            Runs the network forward, starting with x as input.\n",
    "            Returns the activity of the output layer.\n",
    "\n",
    "            All node use \n",
    "            Note: The activation function used for the output layer\n",
    "            depends on what self.Loss is set to.\n",
    "        '''\n",
    "        try: FeedForward\n",
    "        except NameError:\n",
    "            \n",
    "            #========= YOUR IMPLEMENTATION BELOW =========\n",
    "\n",
    "            x = np.array(x)  # Convert input to array, in case it's not\n",
    "\n",
    "            # === YOUR CODE HERE ===\n",
    "            self.lyr[0].h = x\n",
    "            for i in range(0,self.n_layers-1):\n",
    "                # next layer: z = wx + b, y = sigma(z)\n",
    "                # w,x,b from this layer. \n",
    "                self.lyrs[i+1].z = self.W[i].dot(self.lyrs[i].h) + self.lyrs[i+1].b\n",
    "                self.lyrs[i+1].h = self.lyrs[i+1].sigma()\n",
    "            return self.lyrs[-1].h\n",
    "        \n",
    "            #========= YOUR IMPLEMENTATION ABOVE =========\n",
    "                        \n",
    "        else:\n",
    "            return FeedForward(self, x)\n",
    "\n",
    "    \n",
    "    def BackProp(self, t, lrate=0.05):\n",
    "        '''\n",
    "            net.BackProp(targets, lrate=0.05)\n",
    "            \n",
    "            Given the current network state and targets t, updates the connection\n",
    "            weights and biases using the backpropagation algorithm.\n",
    "            \n",
    "            Inputs:\n",
    "             t      an array of targets (number of samples must match the\n",
    "                    network's output)\n",
    "             lrate  learning rate\n",
    "        '''\n",
    "        #====== REMOVE BELOW IF YOU DON'T PLAN TO USE THE SOLUTIONS ======\n",
    "        try: BackProp\n",
    "        except NameError:\n",
    "            \n",
    "            #========= YOUR IMPLEMENTATION BELOW =========\n",
    "            \n",
    "            t = np.array(t)  # convert t to an array, in case it's not\n",
    "\n",
    "            # === YOUR CODE HERE ===\n",
    "\n",
    "                        \n",
    "            #========= YOUR IMPLEMENTATION ABOVE =========\n",
    "\n",
    "        else:\n",
    "            BackProp(self, t, lrate)\n",
    "\n",
    "            \n",
    "    def Learn(self, inputs, targets, lrate=0.05, epochs=1, progress=True):\n",
    "        '''\n",
    "            Network.Learn(data, lrate=0.05, epochs=1, progress=True)\n",
    "\n",
    "            Run through the dataset 'epochs' number of times, incrementing the\n",
    "            network weights after each epoch. For each epoch, it\n",
    "            shuffles the order of the samples.\n",
    "\n",
    "            Inputs:\n",
    "              data is a list of 2 arrays, one for inputs, and one for targets\n",
    "              lrate is the learning rate (try 0.001 to 0.5)\n",
    "              epochs is the number of times to go through the training data\n",
    "              progress (Boolean) indicates whether to show cost\n",
    "        '''\n",
    "        try: Learn\n",
    "        except NameError:\n",
    "            \n",
    "            #========= YOUR IMPLEMENTATION BELOW =========\n",
    "\n",
    "            # === YOUR CODE HERE ===\n",
    "\n",
    "            pass\n",
    "            \n",
    "            #========= YOUR IMPLEMENTATION ABOVE =========\n",
    "\n",
    "        else:\n",
    "            Learn(self, inputs, targets, lrate=lrate, epochs=epochs, progress=progress)\n",
    "    \n",
    "    \n",
    "    def __init__(self, sizes, type='classifier'):\n",
    "        '''\n",
    "            net = Network(sizes, type='classifier')\n",
    "\n",
    "            Creates a Network and saves it in the variable 'net'.\n",
    "\n",
    "            Inputs:\n",
    "              sizes is a list of integers specifying the number\n",
    "                  of nodes in each layer\n",
    "                  eg. [5, 20, 3] will create a 3-layer network\n",
    "                      with 5 input, 20 hidden, and 3 output nodes\n",
    "              type can be either 'classifier' or 'regression', and\n",
    "                  sets the activation function on the output layer,\n",
    "                  as well as the loss function.\n",
    "                  'classifier': logistic, cross entropy\n",
    "                  'regression': linear, mean squared error\n",
    "        '''\n",
    "        self.n_layers = len(sizes)\n",
    "        self.lyr = []    # a list of Layers\n",
    "        self.W = []      # Weight matrices, indexed by the layer below it\n",
    "        \n",
    "        self.cost_history = []  # keeps track of the cost as learning progresses\n",
    "        \n",
    "        # Two common types of networks\n",
    "        # The member variable self.Loss refers to one of the implemented\n",
    "        # loss functions: MSE, or CrossEntropy.\n",
    "        # Call it using self.Loss(t)\n",
    "        if type=='classifier':\n",
    "            self.classifier = True\n",
    "            self.Loss = CrossEntropy\n",
    "            self.gradLoss = gradCrossEntropy\n",
    "            activation = 'logistic'\n",
    "        else:\n",
    "            self.classifier = False\n",
    "            self.Loss = MSE\n",
    "            self.gradLoss = gradMSE\n",
    "            activation = 'identity'\n",
    "\n",
    "        # Create and add Layers (using logistic for hidden layers)\n",
    "        for n in sizes[:-1]:\n",
    "            self.lyr.append( Layer(n) )\n",
    "   \n",
    "        # For the top layer, we use the appropriate activtaion function\n",
    "        self.lyr.append( Layer(sizes[-1], act=activation) )\n",
    "    \n",
    "        # Randomly initialize weight matrices\n",
    "        for idx in range(self.n_layers-1):\n",
    "            m = self.lyr[idx].N\n",
    "            n = self.lyr[idx+1].N\n",
    "            temp = np.random.normal(size=[m,n])/np.sqrt(m)\n",
    "            self.W.append(temp)\n",
    "\n",
    "    def Evaluate(self, inputs, targets):\n",
    "        '''\n",
    "            E = net.Evaluate(data)\n",
    "\n",
    "            Computes the average loss over the supplied dataset.\n",
    "\n",
    "            Inputs\n",
    "             inputs  is an array of inputs\n",
    "             targets is a list of corresponding targets\n",
    "\n",
    "            Outputs\n",
    "             E is a scalar, the average loss\n",
    "        '''\n",
    "        y = self.FeedForward(inputs)\n",
    "        return self.Loss(y, targets)\n",
    "\n",
    "    def ClassificationAccuracy(self, inputs, targets):\n",
    "        '''\n",
    "            a = net.ClassificationAccuracy(data)\n",
    "            \n",
    "            Returns the fraction (between 0 and 1) of correct one-hot classifications\n",
    "            in the dataset.\n",
    "        '''\n",
    "        y = self.FeedForward(inputs)\n",
    "        yb = OneHot(y)\n",
    "        n_incorrect = np.sum(yb!=targets) / 2.\n",
    "        return 1. - float(n_incorrect) / NSamples(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 5 Classes in 8-Dimensional Space\n",
    "np.random.seed(15)\n",
    "noise = 0.1\n",
    "InputClasses = np.array([[1,0,1,0,0,1,1,0],\n",
    "                         [0,1,0,1,0,1,0,1],\n",
    "                         [0,1,1,0,1,0,0,1],\n",
    "                         [1,0,0,0,1,0,1,1],\n",
    "                         [1,0,0,1,0,1,0,1]], dtype=float)\n",
    "OutputClasses = np.array([[1,0,0,0,0],\n",
    "                          [0,1,0,0,0],\n",
    "                          [0,0,1,0,0],\n",
    "                          [0,0,0,1,0],\n",
    "                          [0,0,0,0,1]], dtype=float)\n",
    "n_input = np.shape(InputClasses)[1]\n",
    "n_output = np.shape(OutputClasses)[1]\n",
    "n_classes = np.shape(InputClasses)[0]\n",
    "\n",
    "# Create a training dataset\n",
    "n_samples = 100\n",
    "training_output = []\n",
    "training_input = []\n",
    "for idx in range(n_samples):\n",
    "    k = np.random.randint(n_classes)\n",
    "    x = InputClasses[k,:] + np.random.normal(size=n_input)*noise\n",
    "    t = OutputClasses[k,:]\n",
    "    training_input.append(x)\n",
    "    training_output.append(t)\n",
    "\n",
    "# Create a test dataset\n",
    "n_samples = 100\n",
    "test_output = []\n",
    "test_input = []\n",
    "for idx in range(n_samples):\n",
    "    k = np.random.randint(n_classes)\n",
    "    x = InputClasses[k,:] + np.random.normal(size=n_input)*noise\n",
    "    t = OutputClasses[k,:]\n",
    "    test_input.append(x)\n",
    "    test_output.append(t)\n",
    "\n",
    "train = [np.array(training_input), np.array(training_output)]\n",
    "test = [np.array(test_input), np.array(test_output)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Network\n",
    "net = Network([n_input, 18, n_output], type='classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Network' object has no attribute 'lyrs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-69dfe1a5fffc>\u001b[0m in \u001b[0;36mFeedForward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m         '''\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFeedForward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mNameError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FeedForward' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1a4f68b3dc32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mCE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-69dfe1a5fffc>\u001b[0m in \u001b[0;36mEvaluate\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m    158\u001b[0m              \u001b[0mE\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mscalar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0maverage\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         '''\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeedForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-69dfe1a5fffc>\u001b[0m in \u001b[0;36mFeedForward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0;31m# next layer: z = wx + b, y = sigma(z)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;31m# w,x,b from this layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlyrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlyrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlyrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlyrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlyrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlyrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Network' object has no attribute 'lyrs'"
     ]
    }
   ],
   "source": [
    "CE = net.Evaluate(train[0], train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate it before training\n",
    "CE = net.Evaluate(train[0], train[1])\n",
    "accuracy = net.ClassificationAccuracy(train[0], train[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.Learn(train[0], train[1], epochs=500, lrate=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(net.cost_history);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate it After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Set')\n",
    "CE = net.Evaluate(train[0], train[1])\n",
    "accuracy = net.ClassificationAccuracy(train[0], train[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Set')\n",
    "CE = net.Evaluate(test[0], test[1])\n",
    "accuracy = net.ClassificationAccuracy(test[0], test[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.random.randint(len(test[0]))\n",
    "print(net.FeedForward(test[0][p]))\n",
    "print(test[1][p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Regression Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 1D -> 1D (linear mapping)\n",
    "np.random.seed(846)\n",
    "n_input = 1\n",
    "n_output = 1\n",
    "slope = np.random.rand() - 0.5\n",
    "intercept = np.random.rand()*2. - 1.\n",
    "\n",
    "def myfunc(x):\n",
    "    return slope*x+intercept\n",
    "\n",
    "# Create a training dataset\n",
    "n_samples = 200\n",
    "training_output = []\n",
    "training_input = []\n",
    "xv = np.linspace(-1, 1, n_samples)\n",
    "for idx in range(n_samples):\n",
    "    #x = np.random.rand()*2. - 1.\n",
    "    x = xv[idx]\n",
    "    t = myfunc(x) + np.random.normal(scale=0.1)\n",
    "    training_input.append(np.array([x]))\n",
    "    training_output.append(np.array([t]))\n",
    "\n",
    "# Create a testing dataset\n",
    "n_samples = 50\n",
    "test_input = []\n",
    "test_output = []\n",
    "xv = np.linspace(-1, 1, n_samples)\n",
    "for idx in range(n_samples):\n",
    "    #x = np.random.rand()*2. - 1.\n",
    "    x = xv[idx] + np.random.normal(scale=0.1)\n",
    "    t = myfunc(x) + np.random.normal(scale=0.1)\n",
    "    test_input.append(np.array([x]))\n",
    "    test_output.append(np.array([t]))\n",
    "\n",
    "# Create a perfect dataset\n",
    "n_samples = 100\n",
    "perfect_input = []\n",
    "perfect_output = []\n",
    "xv = np.linspace(-1, 1, n_samples)\n",
    "for idx in range(n_samples):\n",
    "    #x = np.random.rand()*2. - 1.\n",
    "    x = xv[idx]\n",
    "    t = myfunc(x)\n",
    "    perfect_input.append(np.array([x]))\n",
    "    perfect_output.append(np.array([t]))\n",
    "    \n",
    "train = [np.array(training_input), np.array(training_output)]\n",
    "test = [np.array(test_input), np.array(test_output)]\n",
    "perfect = [np.array(perfect_input), np.array(perfect_output)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network([1, 10, 1], type='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate it before training\n",
    "mse = net.Evaluate(train[0], train[1])\n",
    "print('MSE = '+str(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net.Learn(train[0], train[1], epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(net.cost_history);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate it After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On training dataset\n",
    "mse = net.Evaluate(train[0], train[1])\n",
    "print('Training MSE = '+str(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On test dataset\n",
    "mse = net.Evaluate(test[0], test[1])\n",
    "print('Test MSE = '+str(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate our model and the TRUE solution (since we know it)\n",
    "s = np.linspace(-1, 1, 200)\n",
    "y = net.FeedForward(np.array([s]).T)\n",
    "p = [myfunc(x) for x in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training data,\n",
    "# as well as out model and the true model\n",
    "plt.plot(training_input, training_output, 'b.')\n",
    "plt.plot(s,p, 'g--', linewidth=2)\n",
    "plt.plot(s,y, 'r--', linewidth=3)\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
