{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import importlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uncomment these to use the solution instead of your own implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from a2_solutions import FeedForward\n",
    "# from a2_solutions import BackProp\n",
    "from a2_solutions import Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Logistic Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d \\sigma(z)}{d z}\n",
    "  &= \\frac{-e^{-z}}{- \\ (1+ e^{-z})^2} \\\\\n",
    "  &= \\frac{1}{1+ e^{-z}} \\ \\left(1 - \\frac{1}{1+ e^{-z}} \\right) \\\\\n",
    "  &= \\sigma(z) \\ (1 - \\sigma(z))\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help you with $\\LaTeX$, and to show you my expectations, here is a sample taken from the lecture notes, taken from the 3rd and 4th page of the notes entitled \"Error Backpropagation\". It has nothing to do with the solution to this question, but just demonstrates some of the features of $\\LaTeX$. Notice how I include English statments to guide the reader through the derivation.\n",
    "\n",
    "<a target=_new href=\"http://detexify.kirelabs.org/classify.html\">This web page</a> is very handy for identifying $\\LaTeX$ symbols.\n",
    "\n",
    "---\n",
    "More generally, for $\\vec{x} \\in \\mathbb{R}^X$, $\\vec{h} \\in \\mathbb{R}^H$, and $\\vec{y} \\in \\mathbb{R}^Y$.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial \\alpha_i}\n",
    "  &= \\frac{d h_i}{d \\alpha_i} \\\\\n",
    "  &= \\frac{d h_i}{d \\alpha_i}\n",
    "  \\left[ M_{1i} \\ \\cdots \\ M_{Yi} \\right] \\cdot\n",
    "  \\left[ \\frac{\\partial E}{\\partial \\beta_1} \\ \\cdots \\ \\frac{\\partial E}{\\partial \\beta_Y} \\right] \\\\\n",
    "  &= \\frac{d h_i}{d \\alpha_i}\n",
    "   \\left[ M_{1i} \\ \\cdots \\ M_{Yi} \\right]\n",
    "   \\left[ \\begin{array}{c}\n",
    "     \\frac{\\partial E}{\\partial \\beta_1} \\\\\n",
    "     \\vdots \\\\\n",
    "     \\frac{\\partial E}{\\partial \\beta_Y} \\end{array} \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "Thus, for all elements,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left[ \\begin{array}{c}\n",
    "  \\frac{\\partial E}{\\partial \\alpha_1} \\\\\n",
    "  \\vdots \\\\\n",
    "  \\frac{\\partial E}{\\partial \\alpha_H}\n",
    "\\end{array} \\right] &=\n",
    "%\n",
    "\\left[ \\begin{array}{c}\n",
    "  \\frac{d h_1}{d \\alpha_1} \\\\\n",
    "  \\vdots \\\\\n",
    "  \\frac{d h_H}{d \\alpha_H}\n",
    "\\end{array} \\right]\n",
    "\\odot\n",
    "\\left[ \\begin{array}{ccc}\n",
    "  M_{11} & \\cdots & M_{Y1} \\\\\n",
    "  \\vdots & \\ddots & \\vdots \\\\\n",
    "  M_{1H} & \\cdots & M_{YH}\n",
    "\\end{array} \\right]\n",
    "%\n",
    "\\left[ \\begin{array}{c}\n",
    "  \\frac{\\partial E}{\\partial \\beta_1} \\\\\n",
    "  \\vdots \\\\\n",
    "  \\frac{\\partial E}{\\partial \\beta_Y}\n",
    "\\end{array} \\right] \\\\\n",
    "%\n",
    "\\frac{\\partial E}{\\partial \\vec{\\alpha}} &=\n",
    "\\frac{d \\vec{h}}{d \\vec{\\alpha}} \\odot M^\\mathrm{T}\n",
    "\\frac{\\partial E}{\\partial \\vec{\\beta}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q2: Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial z_j}\n",
    "  &= \\sum_{k = 1}^K \\frac{\\partial (t_k \\ln{y_k})}{\\partial z_j}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "If $k = j$:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial (t_k \\ln{y_k})}{\\partial z_j}\n",
    "  &= \\frac{t_k}{y_k} \\frac{e^{z_k} \\ (\\sum_{j=1}^K e^{z_j} - e^{z_k})}{(\\sum_{j=1}^K e^{z_j})^2} \\\\\n",
    "  &= t_k (1 - y_k)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If $k \\neq j$:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial (t_k \\ln{y_k})}{\\partial z_j}\n",
    "  &= \\frac{t_k}{y_k} \\frac{- e^{z_k} e^{z_j}}{(\\sum_{j=1}^K e^{z_j})^2} \\\\\n",
    "  &= - t_k \\ y_j\n",
    "\\end{align}\n",
    "$$\n",
    "Therefore, \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial z_j}\n",
    "  &= \\sum_{k = 1}^K \\frac{\\partial (t_k \\ln{y_k})}{\\partial z_j}\\\\\n",
    "  &= y_j \\ \\sum_{k = 1}^K t_k - t_j\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q3: Top-Layer Error Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "(a)\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial z}\n",
    "  &= \\frac{\\partial E}{\\partial y} \\ \\frac{d y}{d z} \\\\\n",
    "  &= \\frac{d y}{d z} \\ (\\frac{-t}{y} + \\frac{1-t}{1-y}) \\\\\n",
    "  &= y \\ (1-y) \\ (\\frac{-t}{y} + \\frac{1-t}{1-y}) \\\\\n",
    "  &= y - t \\\\\n",
    "  &= \\sigma(z) - t\n",
    "\\end{align}\n",
    "$$  \n",
    "  \n",
    "  \n",
    "(b)\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial z}\n",
    "  &= \\frac{\\partial E}{\\partial y} \\ \\frac{d y}{d z} \\\\\n",
    "  &= \\frac{d y}{d z} \\ (y-t) \\\\\n",
    "  &= z - t\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Q4: Implementing Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplied Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Supplied functions\n",
    "\n",
    "def NSamples(x):\n",
    "    '''\n",
    "        n = NSamples(x)\n",
    "        \n",
    "        Returns the number of samples in a batch of inputs.\n",
    "        \n",
    "        Input:\n",
    "         x   is a 2D array\n",
    "        \n",
    "        Output:\n",
    "         n   is an integer\n",
    "    '''\n",
    "    return len(x)\n",
    "\n",
    "def OneHot(z):\n",
    "    '''\n",
    "        y = OneHot(z)\n",
    "\n",
    "        Applies the one-hot function to the vectors in z.\n",
    "        Example:\n",
    "          OneHot([[0.9, 0.1], [-0.5, 0.1]])\n",
    "          returns np.array([[1,0],[0,1]])\n",
    "\n",
    "        Input:\n",
    "         z    is a 2D array of samples\n",
    "\n",
    "        Output:\n",
    "         y    is an array the same shape as z\n",
    "    '''\n",
    "    y = []\n",
    "    \n",
    "    # Locate the max of each row\n",
    "    for zz in z:\n",
    "        idx = np.argmax(zz)\n",
    "        b = np.zeros_like(zz)\n",
    "        b[idx] = 1.\n",
    "        y.append(b)\n",
    "    y = np.array(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grading:\n",
    "# [1] Divide each by NSamples(t) or NSamples(y) to get the mean\n",
    "# Plus one mark for each of the 4 formulas, as indicated below.\n",
    "def CrossEntropy(y, t):\n",
    "    '''\n",
    "        E = CrossEntropy(y, t)\n",
    "\n",
    "        Evaluates the mean cross entropy loss between outputs y and targets t.\n",
    "\n",
    "        Inputs:\n",
    "          y is an array holding the network outputs\n",
    "          t is an array holding the corresponding targets\n",
    "\n",
    "        Outputs:\n",
    "          E is the mean CE\n",
    "    '''\n",
    "    \n",
    "    # === YOUR CODE HERE ===\n",
    "    n = NSamples(y)\n",
    "    ce = - np.sum(t * np.log(y) + (1-t)* np.log(1-y))/n\n",
    "    return ce\n",
    "\n",
    "def gradCrossEntropy(y, t):\n",
    "    '''\n",
    "        E = gradCrossEntropy(y, t)\n",
    "\n",
    "        Given targets t, evaluates the gradient of the mean cross entropy loss\n",
    "        with respect to the output y.\n",
    "\n",
    "        Inputs:\n",
    "          y is the array holding the network's output\n",
    "          t is an array holding the corresponding targets\n",
    "\n",
    "        Outputs:\n",
    "          dEdy is the gradient of CE with respect to output y\n",
    "    '''\n",
    "    \n",
    "    # === YOUR CODE HERE ===\n",
    "    n = NSamples(y)\n",
    "    dEdy = (- t/y + (1-t)/(1-y))/n\n",
    "    return dEdy\n",
    "\n",
    "def MSE(y, t):\n",
    "    '''\n",
    "        E = MSE(y, t)\n",
    "\n",
    "        Evaluates the mean squared error loss between outputs y and targets t.\n",
    "\n",
    "        Inputs:\n",
    "          y is the array holding the network's output\n",
    "          t is an array holding the corresponding targets\n",
    "\n",
    "        Outputs:\n",
    "          E is the MSE\n",
    "    '''\n",
    "    \n",
    "    # === YOUR CODE HERE ===\n",
    "    n = NSamples(y)\n",
    "    mse = 0.5 * np.sum((y-t)**2)/n    \n",
    "    return mse\n",
    "\n",
    "def gradMSE(y, t):\n",
    "    '''\n",
    "        E = gradMSE(y, t)\n",
    "\n",
    "        Given targets t, evaluates the gradient of the mean squared error loss\n",
    "        with respect to the output y.\n",
    "\n",
    "        Inputs:\n",
    "          y is the array holding the network's output\n",
    "          t is an array holding the corresponding targets\n",
    "\n",
    "        Outputs:\n",
    "          dEdy is the gradient of MSE with respect to output y\n",
    "    '''\n",
    "    \n",
    "    # === YOUR CODE HERE ===\n",
    "    n = NSamples(y)\n",
    "    dEdy = (y-t)/n    \n",
    "    return dEdy\n",
    "\n",
    "#================================================================\n",
    "#\n",
    "#  UNCOMMENT THE CORRESPONDING LINES BELOW IF YOU WANT TO USE\n",
    "#  THE SOLUTIONS INSTEAD OF YOUR VERSION.\n",
    "#\n",
    "#================================================================\n",
    "# from a2_solutions import CrossEntropy\n",
    "# from a2_solutions import gradCrossEntropy\n",
    "# from a2_solutions import MSE\n",
    "# from a2_solutions import gradMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \n",
    "    def __init__(self, n_nodes, act='logistic'):\n",
    "        '''\n",
    "            lyr = Layer(n_nodes, act='logistic')\n",
    "            \n",
    "            Creates a layer object.\n",
    "            \n",
    "            Inputs:\n",
    "             n_nodes  the number of nodes in the layer\n",
    "             act      specifies the activation function\n",
    "                      Use 'logistic' or 'identity'\n",
    "        '''\n",
    "        self.N = n_nodes  # number of nodes in this layer\n",
    "        self.h = []       # node activities\n",
    "        self.z = []\n",
    "        self.b = np.zeros(self.N)  # biases\n",
    "        \n",
    "        # Activation functions\n",
    "        self.sigma = self.Logistic\n",
    "        self.sigma_p = (lambda : self.Logistic_p())\n",
    "        if act=='identity':\n",
    "            self.sigma = self.Identity\n",
    "            self.sigma_p = (lambda : self.Identity_p())\n",
    "       \n",
    "    def Logistic(self):\n",
    "        \n",
    "        return 1. / (1. + np.exp(-self.z))\n",
    "    def Logistic_p(self):\n",
    "        return self.h * (1.-self.h)\n",
    "    def Identity(self):\n",
    "        return self.z\n",
    "    def Identity_p(self):\n",
    "        return np.ones_like(self.h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4(b,c,d) Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "code_folding": [
     52,
     78,
     94,
     106,
     126
    ]
   },
   "outputs": [],
   "source": [
    "class Network():\n",
    "\n",
    "    \n",
    "    def FeedForward(self, x):\n",
    "        '''\n",
    "            y = net.FeedForward(x)\n",
    "\n",
    "            Runs the network forward, starting with x as input.\n",
    "            Returns the activity of the output layer.\n",
    "\n",
    "            All node use \n",
    "            Note: The activation function used for the output layer\n",
    "            depends on what self.Loss is set to.\n",
    "        '''\n",
    "        try: FeedForward\n",
    "        except NameError:\n",
    "            \n",
    "            #========= YOUR IMPLEMENTATION BELOW =========\n",
    "\n",
    "            x = np.array(x)  # Convert input to array, in case it's not\n",
    "\n",
    "            # === YOUR CODE HERE ===\n",
    "            self.lyr[0].h = x\n",
    "            for i in range(self.n_layers - 1):\n",
    "                # z = wT x + bias\n",
    "                self.lyr[i+1].z = (self.lyr[i].h).dot(self.W[i]) + self.lyr[i+1].b\n",
    "                # y = sigma(z)\n",
    "                self.lyr[i+1].h = self.lyr[i+1].sigma()\n",
    "            return self.lyr[-1].h\n",
    "        \n",
    "            #========= YOUR IMPLEMENTATION ABOVE =========\n",
    "                        \n",
    "        else:\n",
    "            return FeedForward(self, x)\n",
    "\n",
    "    \n",
    "    def BackProp(self, t, lrate=0.05):\n",
    "        '''\n",
    "            net.BackProp(targets, lrate=0.05)\n",
    "            \n",
    "            Given the current network state and targets t, updates the connection\n",
    "            weights and biases using the backpropagation algorithm.\n",
    "            \n",
    "            Inputs:\n",
    "             t      an array of targets (number of samples must match the\n",
    "                    network's output)\n",
    "             lrate  learning rate\n",
    "        '''\n",
    "        #====== REMOVE BELOW IF YOU DON'T PLAN TO USE THE SOLUTIONS ======\n",
    "        try: BackProp\n",
    "        except NameError:\n",
    "            \n",
    "            #========= YOUR IMPLEMENTATION BELOW =========\n",
    "            \n",
    "            t = np.array(t)  # convert t to an array, in case it's not\n",
    "\n",
    "            # === YOUR CODE HERE ===\n",
    "            #\n",
    "            n = NSamples(self.lyr[-1].h)\n",
    "            dEdz = (self.lyr[-1].h - t)/n\n",
    "            for i in range(self.n_layers - 2, -1, -1):\n",
    "                dEdw = (self.lyr[i].h.T).dot(dEdz)\n",
    "                dEdz = self.lyr[i].sigma_p() * dEdz.dot(self.W[i].T)\n",
    "                self.W[i] = self.W[i] - lrate * dEdw\n",
    "                prev_dEdz = dEdz\n",
    "                \n",
    "                        \n",
    "            #========= YOUR IMPLEMENTATION ABOVE =========\n",
    "\n",
    "        else:\n",
    "            BackProp(self, t, lrate)\n",
    "\n",
    "            \n",
    "    def Learn(self, inputs, targets, lrate=0.05, epochs=1, progress=True):\n",
    "        '''\n",
    "            Network.Learn(data, lrate=0.05, epochs=1, progress=True)\n",
    "\n",
    "            Run through the dataset 'epochs' number of times, incrementing the\n",
    "            network weights after each epoch.\n",
    "\n",
    "            Inputs:\n",
    "              data is a list of 2 arrays, one for inputs, and one for targets\n",
    "              lrate is the learning rate (try 0.001 to 0.5)\n",
    "              epochs is the number of times to go through the training data\n",
    "              progress (Boolean) indicates whether to show cost\n",
    "        '''\n",
    "        try: Learn\n",
    "        except NameError:\n",
    "            \n",
    "            #========= YOUR IMPLEMENTATION BELOW =========\n",
    "\n",
    "            # === YOUR CODE HERE ===\n",
    "\n",
    "            pass\n",
    "            \n",
    "            #========= YOUR IMPLEMENTATION ABOVE =========\n",
    "\n",
    "        else:\n",
    "            Learn(self, inputs, targets, lrate=lrate, epochs=epochs, progress=progress)\n",
    "    \n",
    "    \n",
    "    def __init__(self, sizes, type='classifier'):\n",
    "        '''\n",
    "            net = Network(sizes, type='classifier')\n",
    "\n",
    "            Creates a Network and saves it in the variable 'net'.\n",
    "\n",
    "            Inputs:\n",
    "              sizes is a list of integers specifying the number\n",
    "                  of nodes in each layer\n",
    "                  eg. [5, 20, 3] will create a 3-layer network\n",
    "                      with 5 input, 20 hidden, and 3 output nodes\n",
    "              type can be either 'classifier' or 'regression', and\n",
    "                  sets the activation function on the output layer,\n",
    "                  as well as the loss function.\n",
    "                  'classifier': logistic, cross entropy\n",
    "                  'regression': linear, mean squared error\n",
    "        '''\n",
    "        self.n_layers = len(sizes)\n",
    "        self.lyr = []    # a list of Layers\n",
    "        self.W = []      # Weight matrices, indexed by the layer below it\n",
    "        \n",
    "        self.cost_history = []  # keeps track of the cost as learning progresses\n",
    "        \n",
    "        # Two common types of networks\n",
    "        # The member variable self.Loss refers to one of the implemented\n",
    "        # loss functions: MSE, or CrossEntropy.\n",
    "        # Call it using self.Loss(t)\n",
    "        if type=='classifier':\n",
    "            self.classifier = True\n",
    "            self.Loss = CrossEntropy\n",
    "            self.gradLoss = gradCrossEntropy\n",
    "            activation = 'logistic'\n",
    "        else:\n",
    "            self.classifier = False\n",
    "            self.Loss = MSE\n",
    "            self.gradLoss = gradMSE\n",
    "            activation = 'identity'\n",
    "\n",
    "        # Create and add Layers (using logistic for hidden layers)\n",
    "        for n in sizes[:-1]:\n",
    "            self.lyr.append( Layer(n) )\n",
    "   \n",
    "        # For the top layer, we use the appropriate activtaion function\n",
    "        self.lyr.append( Layer(sizes[-1], act=activation) )\n",
    "    \n",
    "        # Randomly initialize weight matrices\n",
    "        for idx in range(self.n_layers-1):\n",
    "            m = self.lyr[idx].N\n",
    "            n = self.lyr[idx+1].N\n",
    "            temp = np.random.normal(size=[m,n])/np.sqrt(m)\n",
    "            self.W.append(temp)\n",
    "\n",
    "    def Evaluate(self, inputs, targets):\n",
    "        '''\n",
    "            E = net.Evaluate(data)\n",
    "\n",
    "            Computes the average loss over the supplied dataset.\n",
    "\n",
    "            Inputs\n",
    "             inputs  is an array of inputs\n",
    "             targets is a list of corresponding targets\n",
    "\n",
    "            Outputs\n",
    "             E is a scalar, the average loss\n",
    "        '''\n",
    "        y = self.FeedForward(inputs)\n",
    "        return self.Loss(y, targets)\n",
    "\n",
    "    def ClassificationAccuracy(self, inputs, targets):\n",
    "        '''\n",
    "            a = net.ClassificationAccuracy(data)\n",
    "            \n",
    "            Returns the fraction (between 0 and 1) of correct one-hot classifications\n",
    "            in the dataset.\n",
    "        '''\n",
    "        y = self.FeedForward(inputs)\n",
    "        yb = OneHot(y)\n",
    "        n_incorrect = np.sum(yb!=targets) / 2.\n",
    "        return 1. - float(n_incorrect) / NSamples(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 5 Classes in 8-Dimensional Space\n",
    "np.random.seed(15)\n",
    "noise = 0.1\n",
    "InputClasses = np.array([[1,0,1,0,0,1,1,0],\n",
    "                         [0,1,0,1,0,1,0,1],\n",
    "                         [0,1,1,0,1,0,0,1],\n",
    "                         [1,0,0,0,1,0,1,1],\n",
    "                         [1,0,0,1,0,1,0,1]], dtype=float)\n",
    "OutputClasses = np.array([[1,0,0,0,0],\n",
    "                          [0,1,0,0,0],\n",
    "                          [0,0,1,0,0],\n",
    "                          [0,0,0,1,0],\n",
    "                          [0,0,0,0,1]], dtype=float)\n",
    "n_input = np.shape(InputClasses)[1]\n",
    "n_output = np.shape(OutputClasses)[1]\n",
    "n_classes = np.shape(InputClasses)[0]\n",
    "\n",
    "# Create a training dataset\n",
    "n_samples = 100\n",
    "training_output = []\n",
    "training_input = []\n",
    "for idx in range(n_samples):\n",
    "    k = np.random.randint(n_classes)\n",
    "    x = InputClasses[k,:] + np.random.normal(size=n_input)*noise\n",
    "    t = OutputClasses[k,:]\n",
    "    training_input.append(x)\n",
    "    training_output.append(t)\n",
    "\n",
    "# Create a test dataset\n",
    "n_samples = 100\n",
    "test_output = []\n",
    "test_input = []\n",
    "for idx in range(n_samples):\n",
    "    k = np.random.randint(n_classes)\n",
    "    x = InputClasses[k,:] + np.random.normal(size=n_input)*noise\n",
    "    t = OutputClasses[k,:]\n",
    "    test_input.append(x)\n",
    "    test_output.append(t)\n",
    "\n",
    "train = [np.array(training_input), np.array(training_output)]\n",
    "test = [np.array(test_input), np.array(test_output)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Network\n",
    "net = Network([n_input, 18, n_output], type='classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "CE = net.Evaluate(train[0], train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy = 3.6170513253334455\n",
      "     Accuracy = 26.0%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate it before training\n",
    "CE = net.Evaluate(train[0], train[1])\n",
    "accuracy = net.ClassificationAccuracy(train[0], train[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a2_solutions.Learn\n",
      "Epoch 0: Cost = 3.6170513253334455\n",
      "Epoch 20: Cost = 1.3884153750352246\n",
      "Epoch 40: Cost = 0.6082997439164033\n",
      "Epoch 60: Cost = 0.3428362895658575\n",
      "Epoch 80: Cost = 0.2246446892084974\n",
      "Epoch 100: Cost = 0.16078464114647859\n",
      "Epoch 120: Cost = 0.12256384253859476\n",
      "Epoch 140: Cost = 0.09782280009693572\n",
      "Epoch 160: Cost = 0.08078044752770476\n",
      "Epoch 180: Cost = 0.06845218583614728\n",
      "Epoch 200: Cost = 0.059181221328880776\n",
      "Epoch 220: Cost = 0.05198868650096497\n",
      "Epoch 240: Cost = 0.04626501576425172\n",
      "Epoch 260: Cost = 0.041613446635384264\n",
      "Epoch 280: Cost = 0.037765891469520765\n",
      "Epoch 300: Cost = 0.03453534811426291\n",
      "Epoch 320: Cost = 0.0317877607682535\n",
      "Epoch 340: Cost = 0.029424727929444713\n",
      "Epoch 360: Cost = 0.02737251219758923\n",
      "Epoch 380: Cost = 0.025574845573697536\n",
      "Epoch 400: Cost = 0.02398809470858196\n",
      "Epoch 420: Cost = 0.022577935631133878\n",
      "Epoch 440: Cost = 0.02131701868403571\n",
      "Epoch 460: Cost = 0.02018329792441282\n",
      "Epoch 480: Cost = 0.019158815612248727\n"
     ]
    }
   ],
   "source": [
    "net.Learn(train[0], train[1], epochs=500, lrate=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(net.cost_history);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate it After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "a2_solutions.FeedForward\n",
      "a2_solutions.FeedForward\n",
      "Cross Entropy = 0.017661661308027522\n",
      "     Accuracy = 100.0%\n"
     ]
    }
   ],
   "source": [
    "print('Training Set')\n",
    "CE = net.Evaluate(train[0], train[1])\n",
    "accuracy = net.ClassificationAccuracy(train[0], train[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set\n",
      "a2_solutions.FeedForward\n",
      "a2_solutions.FeedForward\n",
      "Cross Entropy = 0.018996107802952175\n",
      "     Accuracy = 100.0%\n"
     ]
    }
   ],
   "source": [
    "print('Test Set')\n",
    "CE = net.Evaluate(test[0], test[1])\n",
    "accuracy = net.ClassificationAccuracy(test[0], test[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a2_solutions.FeedForward\n",
      "[6.69824684e-05 9.83005841e-01 1.28793238e-02 2.22796938e-04\n",
      " 2.45456803e-03]\n",
      "[0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "p = np.random.randint(len(test[0]))\n",
    "print(net.FeedForward(test[0][p]))\n",
    "print(test[1][p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Regression Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 1D -> 1D (linear mapping)\n",
    "np.random.seed(846)\n",
    "n_input = 1\n",
    "n_output = 1\n",
    "slope = np.random.rand() - 0.5\n",
    "intercept = np.random.rand()*2. - 1.\n",
    "\n",
    "def myfunc(x):\n",
    "    return slope*x+intercept\n",
    "\n",
    "# Create a training dataset\n",
    "n_samples = 200\n",
    "training_output = []\n",
    "training_input = []\n",
    "xv = np.linspace(-1, 1, n_samples)\n",
    "for idx in range(n_samples):\n",
    "    #x = np.random.rand()*2. - 1.\n",
    "    x = xv[idx]\n",
    "    t = myfunc(x) + np.random.normal(scale=0.1)\n",
    "    training_input.append(np.array([x]))\n",
    "    training_output.append(np.array([t]))\n",
    "\n",
    "# Create a testing dataset\n",
    "n_samples = 50\n",
    "test_input = []\n",
    "test_output = []\n",
    "xv = np.linspace(-1, 1, n_samples)\n",
    "for idx in range(n_samples):\n",
    "    #x = np.random.rand()*2. - 1.\n",
    "    x = xv[idx] + np.random.normal(scale=0.1)\n",
    "    t = myfunc(x) + np.random.normal(scale=0.1)\n",
    "    test_input.append(np.array([x]))\n",
    "    test_output.append(np.array([t]))\n",
    "\n",
    "# Create a perfect dataset\n",
    "n_samples = 100\n",
    "perfect_input = []\n",
    "perfect_output = []\n",
    "xv = np.linspace(-1, 1, n_samples)\n",
    "for idx in range(n_samples):\n",
    "    #x = np.random.rand()*2. - 1.\n",
    "    x = xv[idx]\n",
    "    t = myfunc(x)\n",
    "    perfect_input.append(np.array([x]))\n",
    "    perfect_output.append(np.array([t]))\n",
    "    \n",
    "train = [np.array(training_input), np.array(training_output)]\n",
    "test = [np.array(test_input), np.array(test_output)]\n",
    "perfect = [np.array(perfect_input), np.array(perfect_output)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network([1, 10, 1], type='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate it before training\n",
    "mse = net.Evaluate(train[0], train[1])\n",
    "print('MSE = '+str(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net.Learn(train[0], train[1], epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(net.cost_history);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate it After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On training dataset\n",
    "mse = net.Evaluate(train[0], train[1])\n",
    "print('Training MSE = '+str(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On test dataset\n",
    "mse = net.Evaluate(test[0], test[1])\n",
    "print('Test MSE = '+str(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate our model and the TRUE solution (since we know it)\n",
    "s = np.linspace(-1, 1, 200)\n",
    "y = net.FeedForward(np.array([s]).T)\n",
    "p = [myfunc(x) for x in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training data,\n",
    "# as well as out model and the true model\n",
    "plt.plot(training_input, training_output, 'b.')\n",
    "plt.plot(s,p, 'g--', linewidth=2)\n",
    "plt.plot(s,y, 'r--', linewidth=3)\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
