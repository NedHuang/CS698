{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import Network as Network\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q1: Vanishing Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\sigma'(x) =\\sigma'(x)*(1-\\sigma'(x)) \\hspace{1cm} \\sigma(x) \\in (0,1)$$\n",
    "Since $$(a-b)^{2} = a^{2} - 2ab + b^{2}\\le 0$$we get\n",
    "\n",
    "$$ab \\le \\frac{a^{2} + b^{2}}{2}$$ which is $$a^{2} + b^{2} \\le 2ab$$ or $$(a+b)^2 \\le 4ab$$\n",
    "In addition, $\\sigma'(x)*(1-\\sigma'(x)) = 1$\n",
    "\n",
    "$$\\left[\\sigma'(x)+(1-\\sigma'(x))\\right)^2 = 1^{2} \\ge  4\\sigma'(x)*(1-\\sigma'(x)) $$ \n",
    "$$\\sigma'(x)*(1-\\sigma'(x)) \\le \\frac{1}{4}$$\n",
    "By Cauchy–Schwarz inequality, $$\\|w\\sigma'(wh+b)\\| \\le \\|w\\|\\|\\sigma'(wh+b)\\|$$\n",
    "That is $$\\|\\sigma'(wh+b)\\| \\le \\|w\\| * \\frac{1}{4}$$\n",
    "Therefore, in order to have $$\\|\\sigma'(wh+b)\\| \\ge 1$$\n",
    "we must have $$\\|w\\| * \\frac{1}{4} \\ge 1$$\n",
    "which gives\n",
    "$$\\|w\\| \\ge 4$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\|w\\sigma'(wh+b) \\| \\ge 1$$\n",
    "$$\\sigma'(wh+b) = \\sigma(wh+b) (1-\\sigma(wh+b)) \\hspace{1cm} \\sigma(x) = \\frac{exp(x)}{1+exp(x)}$$ \n",
    "By By Cauchy–Schwarz inequality,\n",
    "$$\\|\\sigma'(wh+b)\\| \\le \\|w\\|\\|\\sigma'(wh+b)\\|$$\n",
    "we have \n",
    "$$\\|w\\|\\|\\sigma'(wh+b)\\| \\ge 1 $$\n",
    "Lets say $X = \\sigma(wh+b)$\n",
    "$$ \\|w\\| \\| \\frac{X}{(1+X)^2}\\| \\ge 1$$\n",
    "Expand and solve for X\n",
    "$$X^2 + 2X + 1 \\le \\|W\\|X $$\n",
    "which is\n",
    "$$X^{2} + (2-\\|w\\|)X + 1 \\le 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solve for x we get\n",
    "$$\\frac{(\\|w\\| - 2) -\\sqrt{(2-\\|W\\|)^2 - 4}}{2} \\le X \\le \\frac{(\\|w\\| - 2) +\\sqrt{(2-\\|W\\|)^2 - 4}}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simplify the expression, we get\n",
    "$$\\frac{\\|w\\|\\left(1 -\\sqrt{1 - \\frac{4}{\\|W\\|}}\\right)-2}{2} \\le X \\le \\frac{\\|w\\|\\left(1 -\\sqrt{1 - \\frac{4}{\\|W\\|}}\\right)+2}{2}$$\n",
    "$$\\frac{\\|w\\|\\left(1 -\\sqrt{1 - \\frac{4}{\\|W\\|}}\\right)}{2}-1 \\le X \\le \\frac{\\|w\\|\\left(1 -\\sqrt{1 - \\frac{4}{\\|W\\|}}\\right)}{2}+1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bubstitute X by $exp(wh+b)$ and solve for x\n",
    "$$\\frac{\\|w\\|\\left(1 -\\sqrt{1 - \\frac{4}{\\|W\\|}}\\right)}{2}-1 \\le e^{wh+b} \\le \\frac{\\|w\\|\\left(1 +\\sqrt{1 - \\frac{4}{\\|W\\|}}\\right)}{2}+1$$\n",
    "\n",
    "$$\\ln{ \\left(\\frac{\\|w\\|\\left(1 -\\sqrt{1 - \\frac{4}{\\|W\\|}}\\right)}{2}-1\\right)} \\le wh+b \\le \\ln{ \\left(\\frac{\\|w\\|\\left(1 +\\sqrt{1 - \\frac{4}{\\|W\\|}}\\right)}{2}+1 \\right)}$$\n",
    "\n",
    "</br>\n",
    "$$\\frac{1}{\\|w\\|}\\left(\\ln{ \\left(\\frac{\\|w\\|\\left(1 -\\sqrt{1 - \\frac{4}{\\|W\\|}}\\right)}{2}-1\\right)}-b\\right) \\le h \\le \\frac{1}{\\|w\\|}\\left(\\ln{ \\left(\\frac{\\|w\\|\\left(1 +\\sqrt{1 - \\frac{4}{\\|W\\|}}\\right)}{2}+1\\right)}-b\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interval equals the difference between the upper boundary and the lower boundary\n",
    "$$d = \\frac{1}{\\|w\\|}\\left(\\ln{ \\left(\\frac{\\|w\\|\\left(1 +\\sqrt{1 - \\frac{4}{\\|W\\|}}\\right)}{2}+1\\right)}-b\\right) - \\frac{1}{\\|w\\|}\\left(\\ln{ \\left(\\frac{\\|w\\|\\left(1 -\\sqrt{1  \\frac{4}{\\|W\\|}}\\right)}{2}-1\\right)}-b\\right)$$\n",
    "$$d =\\frac{1}{\\|w\\|}\\ln\\left( \\frac{\\|w\\|\\left(1 +\\sqrt{1 - \\frac{4}{\\|W\\|}}\\right)-2}{\\|w\\|\\left(1-\\sqrt{1 - \\frac{4}{\\|W\\|}}\\right)-2}\\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multiply fraction $f$ inside the log by $\\|w\\|\\left(1 +\\sqrt{1 - \\frac{4}{\\|W\\|}}\\right)-2$ to simplify it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f=\\frac{\\|w\\|\\left(1 +\\sqrt{1 - \\frac{4}{\\|W\\|}}\\right)-2}{\\|w\\|\\left(1-\\sqrt{1 - \\frac{4}{\\|W\\|}}\\right)-2}=\n",
    "\\frac{\\left(\\|w\\|\\left(1 +\\sqrt{1 - \\frac{4}{\\|W\\|}}\\right)-2\\right)\\left(\\|w\\|\\left(1 +\\sqrt{1 - \\frac{4}{\\|W\\|}}\\right)-2\\right)}{\\left(\\|w\\|\\left(1-\\sqrt{1 - \\frac{4}{\\|W\\|}}\\right)-2\\right)\\left(\\|w\\|\\left(1 +\\sqrt{1 - \\frac{4}{\\|W\\|}}\\right)-2\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f = \\frac{\\left(\\|w\\|\\left(1+\\sqrt{1 - \\frac{4}{\\|W\\|}}\\right)-2\\right)^2}{\\|w\\|^{2}\\left(1-1-\\frac{4}{\\|w\\|}\\right)-2\\cdot2\\cdot\\|w\\| + 4}=\\frac{\\left(\\|w\\|\\left(1+\\sqrt{1 - \\frac{4}{\\|W\\|}}\\right)-2\\right)^2}{4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$d = \\frac{1}{\\|w\\|}\\ln\\left(f\\right) = \\frac{1}{\\|w\\|}\\ln\\left[\\left(\\frac{\\|w\\|\\left(1+\\sqrt{1 - \\frac{4}{\\|W\\|}}\\right)-2}{2}\\right)^2\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$d = \\frac{1}{\\|w\\|}\\ln\\left(f\\right) = \\frac{2}{\\|w\\|}\\ln\\left[\\frac{\\|w\\|\\left(1+\\sqrt{1 - \\frac{4}{\\|W\\|}}\\right)}{2}-1\\right]=\\frac{2}{\\|w\\|}\\ln\\left[\\frac{\\|w\\|}{2}\\left(1+\\sqrt{1 - \\frac{4}{\\|W\\|}}\\right)-1\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## (c) Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# [1] Evaluate expression\n",
    "def blah(w):\n",
    "    aw = abs(w)\n",
    "    v = 2./aw * np.log( (aw*(1+np.sqrt(1-4./aw)))/2. - 1 )\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# [1] Evaluate and plot expression\n",
    "w = np.linspace(4.1, 15, 200)\n",
    "v = [blah(ww) for ww in w]\n",
    "plt.plot(w,v)\n",
    "plt.xlabel(r'$|w|$')\n",
    "plt.ylabel('Width of values');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# [1] Show that peak is around 6.9\n",
    "w = np.linspace(6.8,7,50)\n",
    "v = [blah(ww) for ww in w]\n",
    "plt.plot(w,v)\n",
    "plt.xlabel(r'$|w|$')\n",
    "plt.ylabel('Width of values');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: Weight Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def GenerateDatasets(n, seed=None):\n",
    "    # 1D -> 1D (line mapping)\n",
    "    n_input = 1\n",
    "    n_output = 1\n",
    "    noise = 0.5\n",
    "\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "    a = np.random.rand()\n",
    "    m = np.random.rand() + 0.5\n",
    "    b = np.random.rand()-0.5\n",
    "\n",
    "    def myfunc(x):\n",
    "        return a*x**2 + m*x + b\n",
    "\n",
    "    # Create a training dataset\n",
    "    n_samples = n\n",
    "    training_output = []\n",
    "    training_input = []\n",
    "    xv = np.linspace(-1, 1, n_samples)\n",
    "    for idx in range(n_samples):\n",
    "        #x = np.random.rand()*2. - 1.\n",
    "        x = xv[idx]\n",
    "        t = myfunc(x) + np.random.normal(scale=noise)\n",
    "        training_input.append(np.array([x]))\n",
    "        training_output.append(np.array([t]))\n",
    "\n",
    "    # Create a testing dataset\n",
    "    test_input = []\n",
    "    test_output = []\n",
    "    n_test_samples = 300\n",
    "    xv = np.linspace(-1, 1, n_test_samples)\n",
    "    for idx in range(n_test_samples):\n",
    "        #x = np.random.rand()*2. - 1.\n",
    "        x = xv[idx] #+ np.random.normal(scale=0.1)\n",
    "        t = myfunc(x) + np.random.normal(scale=noise)\n",
    "        test_input.append(np.array([x]))\n",
    "        test_output.append(np.array([t]))\n",
    "\n",
    "\n",
    "    train = [np.array(training_input), np.array(training_output)]\n",
    "    test = [np.array(test_input), np.array(test_output)]\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Sample Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Here is an example, creating a Network object.\n",
    "import importlib\n",
    "importlib.reload(Network)\n",
    "net = Network.Network([1,6,1], type='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Generate the training and testing datasets\n",
    "train, test = GenerateDatasets(50, seed=3502)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Train the network\n",
    "progress = net.SGD(train[0], train[1], epochs=100, lrate=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the cost \n",
    "print('Training cost = '+str(net.Evaluate(train[0], train[1])))\n",
    "print('    Test cost = '+str(net.Evaluate(test[0], test[1])))\n",
    "\n",
    "# These results won't necessarily make sense, since the network model\n",
    "# might not be the right choice for the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## (a) Implement weigth decay in `BackProp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class RobustNetwork(Network.Network):\n",
    "        \n",
    "    def BackProp(self, t, lrate=0.05, decay=0.):\n",
    "        '''\n",
    "            net.BackProp(targets, lrate=0.05, decay=0.)\n",
    "            \n",
    "            Given the current network state and targets t, updates the connection\n",
    "            weights and biases using the backpropagation algorithm.\n",
    "            \n",
    "            Inputs:\n",
    "             t      an array of targets (number of samples must match the\n",
    "                    network's output)\n",
    "             lrate  learning rate\n",
    "             decay  is the coefficient for weight/bias decay\n",
    "        '''\n",
    "            \n",
    "        t = np.array(t)  # convert t to an array, in case it's not\n",
    "\n",
    "        # Error gradient for top layer (same for both types)\n",
    "        dEdz = self.TopGradient(t)\n",
    "\n",
    "        # Loop down through the layers\n",
    "        # Start second-from-the-top, and go down to layer 0\n",
    "        for i in range(self.n_layers-2, -1, -1):\n",
    "            pre = self.lyr[i]\n",
    "\n",
    "            # Gradient w.r.t. weights\n",
    "            dEdW = pre.h.T @ dEdz\n",
    "\n",
    "            # Gradient w.r.t. biases\n",
    "            dEdb = np.sum(dEdz, axis=0)\n",
    "\n",
    "            # Use Sigma'\n",
    "            # Project error gradient down to layer below\n",
    "            dEdz = ( dEdz @ self.W[i].T ) * pre.sigma_p()\n",
    "\n",
    "            # Update weights and biases\n",
    "            self.W[i] -= lrate*dEdW\n",
    "            self.lyr[i+1].b -= lrate*dEdb\n",
    "            \n",
    "\n",
    "    \n",
    "    def SGD(self, inputs, targets, lrate=0.05, epochs=1, batch_size=10, decay=0):\n",
    "        '''\n",
    "            progress = net.SGD(inputs, targets, lrate=0.05, epochs=1, batch_size=10, decay=0)\n",
    "\n",
    "            Performs Stochastic Gradient Descent on the network.\n",
    "            Run through the dataset in batches 'epochs' number of times, incrementing the\n",
    "            network weights after each batch. For each epoch, it shuffles the dataset.\n",
    "\n",
    "            Inputs:\n",
    "              inputs  is an array of input samples\n",
    "              targets is a corresponding array of targets\n",
    "              lrate   is the learning rate (try 0.001 to 5)\n",
    "              epochs  is the number of times to go through the training data\n",
    "              batch_size is the number of samples for each batch\n",
    "              decay   is the decay coefficient for the weights and biases\n",
    "              \n",
    "            Outputs:\n",
    "              progress is an (epochs)x2 array with epoch in the first column, and \n",
    "                      cost in the second column\n",
    "        '''\n",
    "        loss_history = []\n",
    "        for k in range(epochs):\n",
    "            batches = Network.MakeBatches(inputs, targets, batch_size=batch_size, shuffle=True)\n",
    "            for mini_batch in batches:\n",
    "                self.FeedForward(mini_batch[0])\n",
    "                self.BackProp(mini_batch[1], lrate=lrate, decay=decay)\n",
    "\n",
    "            loss_history.append([k, self.Evaluate(inputs, targets)])\n",
    "\n",
    "        return np.array(loss_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Create the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RobustNetwork, using MSE and identity output\n",
    "\n",
    "# Make copies -> original_net, dropout_net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Generate a dataset, and train net _without_ decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the training and testing datasets\n",
    "P = 5\n",
    "train, test = GenerateDatasets(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1] Train using SGD (batch_size argument is optional on such a small dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on both datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Train net _with_ decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with dropout, and evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e) Plot training data and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training and Test data\n",
    "\n",
    "# Plot original_net and dropout_net models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f) Average performance over 10 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop for 10 trials\n",
    "trials = 10\n",
    "\n",
    "for k in range(trials):\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean cost over the 10 trials\n",
    "\n",
    "orig_train, decay_train, orig_test, decay_test = 0, 0, 0, 0\n",
    "\n",
    "print('Training set, without decay,   cost = '+str(orig_train))\n",
    "print('Training set, with decay=0.03, cost = '+str(decay_train))\n",
    "print('Test set,     without decay,   cost = '+str(orig_test))\n",
    "print('Test set,     with decay=0.03, cost = '+str(decay_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== Your answer here. ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q3: Classifier Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Helper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     18
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def ClassPlot(x, y):\n",
    "    # FeedForward\n",
    "    yc = Network.OneHot(y)\n",
    "    colour_options = ['y', 'b', 'r', 'g', 'k']\n",
    "    cidx = np.argmax(yc, axis=1)\n",
    "    colours = [colour_options[k] for k in cidx]\n",
    "    plt.scatter(x[:,0],x[:,1], color=colours, marker='.')\n",
    "    plt.axis('equal');\n",
    "\n",
    "def GaussianCluster(mu, sa, N, theta=0.):\n",
    "    theta_rad = theta/180*np.pi\n",
    "    c, s = np.cos(theta_rad), np.sin(theta_rad)\n",
    "    R = np.array([[c,-s],[s,c]])\n",
    "    M = R @ np.diag(sa) @ R.T\n",
    "    y = np.random.multivariate_normal(mu, M, N)\n",
    "    return y\n",
    "\n",
    "def CreateDataset(params):\n",
    "    '''\n",
    "        train, test = CreateDataset(params)\n",
    "        \n",
    "        Creates a dataset using a bunch of Gaussian clouds.\n",
    "        \n",
    "        Inputs:\n",
    "          params is a list of 5-tuples. Each tuple (or list)\n",
    "             describes a 2-D Gaussian cloud:\n",
    "             1) the mean\n",
    "             2) standard deviation along x and y directions\n",
    "             3) rotation of cloud (degrees counter-clockwise)\n",
    "             4) class index\n",
    "             5) number of points\n",
    "             \n",
    "        Outputs:\n",
    "          train is a list containing 2 arrays\n",
    "             - the first array contains the training inputs,\n",
    "               one per row\n",
    "             - the second array contains the corresponding classes\n",
    "          test has the same structure as train, but with\n",
    "               half the number of samples\n",
    "    '''\n",
    "    train_inputs = []\n",
    "    train_outputs = []\n",
    "    test_inputs = []\n",
    "    test_outputs = []\n",
    "    all_c = [p[3] for p in params]\n",
    "    n_classes = np.max(all_c)\n",
    "    for p in params:\n",
    "        xx = GaussianCluster(p[0], p[1], p[4], theta=p[2])\n",
    "        yy = np.zeros((p[4], n_classes+1))\n",
    "        yy[:,p[3]] = 1.\n",
    "        train_inputs.append(xx)\n",
    "        train_outputs.append(yy)\n",
    "        xtest = GaussianCluster(p[0], p[1], int(p[4]/2), theta=p[2])\n",
    "        ytest = np.zeros((int(p[4]/2), n_classes+1))\n",
    "        ytest[:,p[3]] = 1.\n",
    "        test_inputs.append(xtest)\n",
    "        test_outputs.append(ytest)\n",
    "    train = [np.vstack(train_inputs), np.vstack(train_outputs)]\n",
    "    test = [np.vstack(test_inputs), np.vstack(test_outputs)]\n",
    "    return train, test\n",
    "\n",
    "# 0=y, 1=b, 2=r\n",
    "params = [[[0.5, 0.5],   [0.025, 0.01],  -45, 0, 50],\n",
    "          [[-0.5,-0.5],  [0.025, 0.01],  -45, 0, 50],\n",
    "          [[-0.3,-0.35], [0.04,0.01],     10, 0, 80],\n",
    "          [[0.6,0],      [0.01,0.01],      0, 0, 50],\n",
    "          [[0.1,-0.04],  [0.1,0.005],     20, 0, 80],\n",
    "          [[0.5,-0.5],   [0.02,0.01],     45, 1, 50],\n",
    "          [[0,0.6],      [0.01,0.01],      0, 1, 50],\n",
    "          [[-0.5,0.1],   [0.02,0.01],     30, 1, 80],\n",
    "          [[-0.5,0.5],   [0.02,0.01],     45, 2, 50],\n",
    "          [[0,-0.6],     [0.01,0.01],      0, 2, 50],\n",
    "          [[0.2,0.3],    [0.01,0.004],     0, 2, 80],\n",
    "          [[-0.2,0.37],  [0.02,0.005],   -30, 2, 50],\n",
    "          [[0.25,-0.3],  [0.005,0.01],     0, 3, 60],\n",
    "          [[-0.1,0.1],   [0.01,0.01],    -30, 3, 50]\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create an interesting dataset\n",
    "train, test = CreateDataset(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ClassPlot(train[0], train[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## (a) `TopGradient`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ExtNetwork(Network.Network):\n",
    "    \n",
    "    def TopGradient(self, t):\n",
    "        '''\n",
    "            dEdz = net.TopGradient(targets)\n",
    "\n",
    "            Computes and returns the gradient of the cost with respect to the input current\n",
    "            to the output nodes.\n",
    "\n",
    "            Inputs:\n",
    "              targets is a batch of targets corresponding to the last FeedForward run\n",
    "\n",
    "            Outputs:\n",
    "              dEdz is a batch of gradient vectors corresponding to the output nodes\n",
    "        '''\n",
    "        P = len(t)\n",
    "        if self.type=='Bernoulli':\n",
    "            return ( self.lyr[-1].h - t ) / P\n",
    "        elif self.type=='regression':\n",
    "            return ( self.lyr[-1].h - t ) / P\n",
    "        else:\n",
    "            return self.gradLoss(self.lyr[-1].h, t) * self.lyr[-1].sigma_p()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## (b) Create and Evaluate a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a network that can solve the problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train, test = CreateDataset(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Train and evaluate the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## (c) Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Classification accuracy on training and test datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot coloured clouds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q4: Hopfield Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Helper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# The next line sets the default for the number of significant digits to display.\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Utility functions\n",
    "def HammingDist(a, b):\n",
    "    '''\n",
    "     d = HammingDist(a, b)\n",
    "     \n",
    "     Returns the Hamming distance between 'a' and 'b'.\n",
    "     The Hamming distance is the number of elements with opposite signs.\n",
    "     eg. HammingDist([1,1,-1], [-1,1,-1])  -> 1\n",
    "         HammingDist([0.5, -0.1, 1.4], [1, -0.9, 0.1])  -> 0\n",
    "    '''\n",
    "    return np.sum(np.array(a)*np.array(b)<0.)\n",
    "\n",
    "def sigma(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def sigma_inv(y):\n",
    "    return 0.5 * ( np.log(1.+y) - np.log(1.-y) )\n",
    "\n",
    "# Functions for creating a dataset\n",
    "def Sample(Y, flip=-0.5):\n",
    "    '''\n",
    "     yy, ytrue = Sample(Y, flip=-0.5)\n",
    "     \n",
    "     Draw a sample randomly from Y.\n",
    "     With probability 'flip', the sample is contaminated by flipping bits.\n",
    "     If flip is positive, the each bit will be flipped with probability 'flip'.\n",
    "     If flip is negative, then a single random bit will be flipped with prob '-flip'.\n",
    "     eg. flip 1st bit: [1,-1,1,1] -> [-1,-1,1,1]\n",
    "     \n",
    "     Returns:\n",
    "       yy     the (potentially contaminated) sample\n",
    "       ytrue  the corresponding uncontaminated sample\n",
    "    '''\n",
    "    # Choose one of the prototpyes\n",
    "    idx = np.random.randint(len(Y))\n",
    "    yy = copy.deepcopy(Y[idx])\n",
    "    ytrue = copy.deepcopy(yy)\n",
    "    \n",
    "    # Contaminate it\n",
    "    if flip<0:\n",
    "        r = np.random.rand()\n",
    "        if r<abs(flip):\n",
    "            k = np.random.randint(len(yy)) # Choose random bit\n",
    "            yy[k] *= -1.   # Flip it!\n",
    "        \n",
    "    else:\n",
    "        # Flip each bit with prob 'flip'\n",
    "        for k,_ in enumerate(yy):\n",
    "            r = np.random.rand()\n",
    "            if r<flip:\n",
    "                yy[k] *= -1.\n",
    "    return yy, ytrue\n",
    "\n",
    "def MakeDataset(Y, N=1000, flip=-0.5):\n",
    "    '''\n",
    "     Yd, Ytrue = MakeDataset(Y, N=1000, flip=-0.5)\n",
    "     \n",
    "     Creates a dataset of N samples by sampling the prototypes in Y.\n",
    "     For each sample, there is a probability of 'flip' that the sample\n",
    "     will be contaminated (see Sample function).\n",
    "     \n",
    "     Returns:\n",
    "       Yd     the (potentially contaminated) dataset\n",
    "       Ytrue  the corresponding uncontaminated dataset\n",
    "    '''\n",
    "    Yd = []\n",
    "    Ytrue = []\n",
    "    for n in range(N):\n",
    "        yy, ytrue = Sample(Y, flip=flip)\n",
    "        Yd.append(yy)\n",
    "        Ytrue.append(ytrue)\n",
    "    Yd = np.array(Yd)\n",
    "    Ytrue = np.array(Ytrue)\n",
    "    return Yd, Ytrue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Actual Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make a dataset of memories\n",
    "Y = np.array([[1,0,0,1,1,0,0,1],\n",
    "              [0,1,0,1,0,1,0,1],\n",
    "              [1,1,1,0,1,0,1,0],\n",
    "              [0,0,1,0,0,1,1,0]])\n",
    "m = 0.99\n",
    "Y = m*(2.*Y - 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Here is how you can generate 1 (noisy/clean) sample\n",
    "yy, ytrue = Sample(Y, flip=0.1)\n",
    "HammingDist(yy, ytrue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Here is how you can create a (noisy and clean) dataset\n",
    "Yd, Ytrue = MakeDataset(Y, N=10, flip=-0.5)\n",
    "print(Yd)\n",
    "print(Ytrue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## (a) Derive $\\frac{\\partial E}{\\partial y_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "=== Your answer here ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## (b) Derive $\\frac{\\partial E}{\\partial x_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "=== Your answer here ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## (c) Derive $\\frac{\\partial E}{\\partial W_{ij}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "=== Your answer here ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## (d) Derive $\\frac{\\partial E}{\\partial b_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "=== Your answer here ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## (e)-(f) `HopfieldNet` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class HopfieldNet(object):\n",
    "    \n",
    "    def __init__(self, N):\n",
    "        self.N = N   # Number of nodes\n",
    "        \n",
    "        self.lambda_x = 0.1  # decay rate for x\n",
    "        self.lambda_W = 0.1  # decay rate for W\n",
    "        self.lambda_b = 0.1  # decay rate for b\n",
    "        \n",
    "        self.x = np.zeros(N)     # Node input currents\n",
    "        self.b = np.zeros(N)     # Node biases\n",
    "        self.W = np.zeros((N,N)) # Connection weights (symmetric)\n",
    "        \n",
    "        self.sigma = sigma          # Activation function\n",
    "        self.sigma_inv = sigma_inv  # Inverse of activation function\n",
    "    \n",
    "    \n",
    "    \n",
    "    def Train(self, Y, epochs=100, dt=0.01):\n",
    "        '''\n",
    "         HopefieldNet.Train(Y, epochs=100, dt=0.01)\n",
    "         \n",
    "         Train the Hopfield network for epochs, choosing one sample at a time\n",
    "         and using it update the connection weights and biases by taking a\n",
    "         single Euler step of length dt.\n",
    "        '''\n",
    "        for ep in range(epochs):\n",
    "            Ys = copy.deepcopy(Y)\n",
    "            np.random.shuffle(Ys)\n",
    "            \n",
    "            #=== YOUR CODE HERE ===\n",
    "            \n",
    "        \n",
    "        \n",
    "    def Recall(self, T, dt, y):\n",
    "        '''\n",
    "         HopefieldNet.Recall(T, dt, y)\n",
    "         \n",
    "         Run the Hopfield network for T seconds, updating the node activities\n",
    "         using Euler steps of length dt.\n",
    "        '''\n",
    "        self.x = copy.deepcopy(self.sigma_inv(y))\n",
    "        t = 0.\n",
    "\n",
    "        #=== YOUR CODE HERE ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## (g) Train and test the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "Ytrain,_ = MakeDataset(Y, N=1000, flip=0.1)\n",
    "# Create the network\n",
    "net = HopfieldNet(8)\n",
    "# Train it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Test the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Ytest,Ytrue = MakeDataset(Y, N=100, flip=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "T = 5.\n",
    "dt = 0.01\n",
    "correct = 0\n",
    "total = 0\n",
    "for yy,ytrue in zip(Ytest, Ytrue):\n",
    "    net.Recall(T, dt, yy)\n",
    "    if HammingDist(ytrue, net.sigma(net.x))==0:\n",
    "        correct += 1\n",
    "    else:\n",
    "        print('Error: Hamming Dist = '+str(HammingDist(yy, ytrue)))\n",
    "    total += 1\n",
    "print('Success rate = '+str(correct/total*100.)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "yy, ytrue = Sample(Y, flip=0.1)\n",
    "net.Recall(T, dt, yy)\n",
    "print('Hamming Distance = '+str(HammingDist(yy, ytrue)))\n",
    "print(np.vstack((ytrue,yy,net.sigma(net.x))))\n",
    "if not HammingDist(ytrue, net.sigma(net.x)): print('CORRECT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
